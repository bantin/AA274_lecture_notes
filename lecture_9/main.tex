\documentclass[twoside]{article}

\usepackage{enumitem} % for customizing enumerate tags
\usepackage{amsmath,amsthm,amssymb}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{minted}
\usepackage[math]{kurier}
\usepackage[sc]{mathpazo}
\renewcommand{\sfdefault}{kurier}


\usepackage{graphics}
\usepackage{graphicx}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}




\newcommand{\lecture}[3]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf \sffamily AA 274: Principles of Robotic Autonomy
                        \hfill Winter 2018} }
       \vspace{4mm}
       \hbox to 6.28in { {\sffamily{\Large \hfill Lecture #1: #2  \hfill}} }
       \vspace{2mm}
       % \hbox to 6.28in { {\it \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace*{4mm}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%
%document
\begin{document}
%modify this
\lecture{9}{Machine Learning and Modern Visual Recognition Techniques}{}


\section{Introduction}
This lecture provides an introduction to Neural Networks and proceeds to discuss Convolutional Neural Networks which are especially suited to process images. With the advance of technology the amount of data available for any given task has gone up exponentially  in the last several decades. Methods such as logistic regression perform well when the training data set is large. But their performance plateaus out after a point. Neural networks have gained popularity because with huge amounts of data which is currently available, even small Neural Networks are able to outperform traditional learning algorithms by large margins. 

Image recognition and image processing has widespread applications in robotics such as  path identification, obstacle avoidance etc. If we were to use a standard Neural Network architecture to process images, we would need to learn parameters of the order of millions. Convolutional Neural Networks have significantly brought down this number by using special convolution layers which exploits the spatial structure of images. 

The core learning objectives of this lecture are:
\begin{enumerate}
     \item The basics of Neural Networks :
     \begin{itemize}
         \item How Neural Networks mimic the architecture of human brain and how multiple layers of the network work together
         \item How to train a Neural network to do a particular task and where they could go wrong
     \end{itemize}
     \item Convolutional Neural Networks:
     \begin{itemize}
         \item This section covers in depth the different elements of a Convolutional Neural Network and some famous architectures commonly used in the industry 
     \end{itemize}
 \end{enumerate}
 


\section{Neural Network Basics}
Work on Artificial Neural Networks, generally known as just 'Neural Networks', has been inspired by the fact that the human brain processes information in a completely different way than computers. Due to this, human brains are much more efficient at certain tasks, such as vision, language processing, etc. than computers. Artificial neural networks aim to mimic this biological model by employing a large number of simple interconnected processing units or 'neurons'.  We may thus offer the following definition of a neural network viewed as an adaptive machine\cite{haykin}:

A neural network is a massively parallel distributed processor made up of simple processing units that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects:\\
1. Knowledge is acquired by the network from its environment through a learning process.\\
2. Inter-neuron connection strengths, known as synaptic weights, are used to store the acquired knowledge.

\subsection{Perceptron - Analogy to a Biological Neuron}
The figure below shows a simplified diagram of a biological neuron:

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/neuron.png}
\caption{Simplified model of a biological neuron\cite{cs231n-website}}
\label{fig:bio-neuron}
\end{figure}

There are approximately 86 billion neurons in the human nervous system and they are connected with approximately $10^14$ - $10^15$ synapses\cite{cs231n-website}.

The figure below shows a commonly used mathematical model of a neuron:

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/neuron_model.jpeg}
\caption{Mathematical model of a neuron\cite{cs231n-website}}
\label{fig:neuron_model}
\end{figure}

Signals travel along the axons (e.g. x0) and interact multiplicatively (e.g. w0x0) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. w0). The idea is that the synaptic strengths (the weights w) are learnable and control the strength of influence (and its direction: excitatory (positive weight) or inhibitory (negative weight)) of one neuron on another. We can model the firing rate of the neuron with an activation function f, which represents the frequency of the spikes along the axon. Historically, a common choice of activation function was the sigmoid function, but the ReLU function is more commonly used nowadays. This is because the gradient of the sigmoid function becomes small as its value increases which reduces the training speed of the model.


\subsection{Single Layer Network / Logistic Regression}
Inspired by this model of the neuron, a 1 layer neural net can be built which takes in binary inputs and provides binary output by taking a weighted sum and passing it through an activation function (as seen below). The theory is that if these weights are tuned perfectly, then it should be able to classify the inputs correctly.


\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/Logistic_regression.png}
\caption{Single layer neural net\cite{Lecture slides}}
\label{fig:Single layer neural net}
\end{figure}


\subsection{Multi-layer Neural Network / Deep Learning}

We can get more resolution on our classifications by putting multiple of these layers together. In such regard, each layer can "learn" to classify a different part of the input. Take for an example the problem of classifying handwritten digits, in particular classifying the number 3. The first layer can be responsible for classifying a curved top, the second a curved bottom, etc.

A sample is shown below (from : http://www.cs.cmu.edu/~aharley/vis/conv/flat.html)

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/deep_neural_net.png}
\caption{Deep Neural Net for Classifying Handwritten Numbers\cite{}}
\label{fig:Single layer neural net}
\end{figure}

Representing this is simply a bunch of single layer neural networks linked together where the output of 1 layer is the input of the second layer.

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/deep_learning.png}
\caption{Deep Learning\cite{}}
\label{fig:Deep Learning}
\end{figure}

\subsection{Activation Functions}

If we didn't use non-linear activation functions, the output of the neural network would just be a linear function of the input. Such a network is basically just a Linear regression Model and and no matter how many layer and nodes we add to such a network, it is equivalent to having a network with a single node.\\

Hence, we use Activation functions to make the network more powerful give it the ability to represent non-linear complex functional mappings between inputs and outputs.

Another important feature of an Activation function is that it should be differentiable. This is because we perform back-propagation to calculate the gradients of the Loss function with respect to the weights and then accordingly optimize weights using gradient descend or any other Optimization technique to decrease the Loss Function.

The following are the most commonly used activation functions:

\begin{enumerate}

\item \textbf{Sigmoid Function:}
The Sigmoid Function takes a real-valued number and maps it to a range between 0 and 1.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/sigmoid.jpeg}
\caption{The sigmoid function\cite{cs231n-website}}
\label{fig:sigmoid}
\end{figure}
\begin{equation*}
f(x) = \frac{1}{1+e^{-x}}
\end{equation*}
Sigmoid Activation functions have historically been used extensively but have now fallen out of favor due to the following drawbacks:
\begin{enumerate}
\item They are prone to saturate as the gradients are very small away from the center. When this occurs, almost no signal will flow through the neuron to its weights and recursively to its data.
\item They are not zero-centered.
\end{enumerate}

\item \textbf{Tanh function:}
The Tanh function looks quite similar to a sigmoid function. It maps a real-valued number to a range between -1 and 1.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/tanh.jpeg}
\caption{The tanh function\cite{cs231n-website}}
\label{fig:tanh}
\end{figure}
\begin{equation*}
f(x) = \tanh(x)
\end{equation*}
It saturates in the same way as a sigmoid does but it has the advantage of being zero-centered. Hence, in practice, it is always preferred to a sigmoid function.

\item \textbf{ReLU Function:}
The Rectified Linear Unit is simply a threshold at zero and has become very popular in the last few years.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/tanh.jpeg}
\caption{The ReLU function\cite{cs231n-website}}
\label{fig:ReLU}
\end{figure}
\begin{equation*}
f(x) = max( 0, x )
\end{equation*}
It has the following advantages:
\begin{enumerate}
\item It greatly accelerates the convergence of stochastic gradient descent compared to tanh/sigmoid.
\item It is computationally cheaper to implement than tanh/sigmoid as it can implemented by simply thresholding a matrix of activations at zero.
\end{enumerate}
The main disadvantage of the ReLU is that ReLU units can irreversibly die during training since they can get knocked off the data manifold.

\item \textbf{Leaky ReLU Function:}
The Leaky ReLU attempts to fix the problem of dying nuerons by providing a small negative slope in the $x<0$ region.
\begin{figure}[!htb]
\centering
\includegraphics[height = 1.2in]{pics/Leaky.png}
\caption{The Leaky ReLU function\cite{cs231n-website}}
\label{fig:ReLU}
\end{figure}
\begin{equation*}
f(x) = max( 0.1 x, x )
\end{equation*}

\end{enumerate}
\subsection{Training Neural Networks}
We now need a way to teach these neural networks weights to make correct predictions. This is done by running the network on known data points and updating the weights of the neurons based on the correctness of the model in classifying the input. \\

Running this however on each input one at a time can take a long period of time for training. Instead these models are run on a sample batch of data. In particular the steps are the following.\\

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/training_net.png}
\caption{Training a neural net\cite{}}
\label{fig:Training a neural net}
\end{figure}

\textbf{1. Sample a batch of data.}
We can use a smaller batch of data at each iteration to train the model to decrease training time.

\textbf{2. Run the input it through the graph and compute the loss.}
The loss function provides serves as a quality metric and provides us an idea on the error of our model. That is, how correct it is.

\textbf{3. Backpropagate}
Activation functions are chosen to be differentiable to make this step easier. Backpropagation is done by taking the gradient with respect to the weights of each neuron in each layer.

\textbf{4. Update these parameters using SGD (Stochastic Gradient Descent)}
Update each parameter using the gradient calculated in the previous step

\textbf{5. Repeat many times}

\subsection{Overfitting}
Creating complex models sometimes have the disadvantage of being extremely accurate only on the test data. That is, the model is able to classify its entire training data with high accuracy however is unable to generalize and fails with test / real data.

On the other hand too simple models are unable to learn the more subtle trends in the data.

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/overfitting.png}
\caption{Overfitting\cite{}}
\label{fig:Overfitting}
\end{figure}
http://mlwiki.org/index.php/Overfitting

In the example above the left model was too simple and was unable to classify some of the values correctly. On the other the right model despite perfectly classifying on this training dataset is unlikely to perform as well in real-world situations. The middle offers a good classification.

To combat overfitting, regularizaiton is usually introduced in models. Regularization is a technique that penalizes complex models or prevents them from being made. Some of these techniques include

\textbf{Dropout}
Randomly select neurons that will not be activated at each pass during training.

\textbf{L2 Regularization}
Penalize the square magnitude of all parameters which forces neuron size to be smaller.

\textbf{Max Norm Constraints}
Limit the maximum weight of a neuron.


%%%%%%%%%%%%
% PART III %
%%%%%%%%%%%%
\section{Convolutional Neural Networks}
A simple Convolutional Neural Networks is a sequence of layers, and every layer of a Convolutional Neural Networks transforms one volume of activations to another through a differentiable function. There are four main types of layers to build ConvNet architectures: \textbf{Convolutional Layer}, \textbf{Nonlinearity Layer}, \textbf{Pooling Layer}, and \textbf{Fully-Connected Layer}. These layers are stacked to form a full ConvNet architecture. In this way, Convolutional Neural Networks transform the original image layer by layer from the original pixel values to the final class scores.

In this section, we mainly discuss the three types of layer in Convolutional Neural Networks, including Convolutional layer, Pooling layer and Fully-connected layer.

\subsection{Convolutional Layer}
\textbf{Parameter Sharing}
Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. It is hard to make perceptrons scalable when the input image size is large given the fact the number of parameters grow quickly with the input size. However, we can dramatically reduce the number of parameters by making one reasonable assumption. If we know the input is image data, we can assume some spatial symmetries. In other word, if one feature is useful to compute at some spatial position $(x,y)$, then it should also be useful to compute at a different position $(x_2,y_2)$, so same parameters could be used.

\textbf{Convolution Details}
The convolution operation essentially performs dot products between the filters and local regions of the input. In other words, each element is computed by element-wise multiplying the local regions of the input (blue part in Figure \ref{fig:conv_demo}) with the filter (red part in Figure \ref{fig:conv_demo}), summing it up, and then offsetting the result by the bias. The output is the green part in Figure \ref{fig:conv_demo}.

Noticing we give the demo in 2D format. In real cases, the input would be a 3D tensor while he depth of the output volume is a hyperparameter. It corresponds to the number of filters we would like to use, each learning to look for something different in the input. Take Figure \ref{fig:conv_demo2} as an example, if we have 6 $5\times5$ filters, we will get 6 separate activation maps as output.

\textbf{Spatial arrangement} Besides depth, there are two hyperparameters to control the size of the output volume: the stride and zero-padding
\begin{itemize}
\item \textbf{stride} Stride specifies how we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around.
\item \textbf{zero-padding} In some cases, the size of filters and stride don’t “fit” neatly and symmetrically across the input. For example, if the input size $H=W=10$ while the filter size is $F=3$ and it takes stride $S=3$, it would be impossible to use stride $S=2$, since
\begin{equation}
\begin{aligned}
(H-F)/S+1&=(10-3)/3+1=3.33~\text{(Not an integer)}\\
(W-F)/S+1&=(10-3)/3+1=3.33~\text{(Not an integer)}\\
\end{aligned}
\end{equation}
Therefore, we need to use zero-padding. We add additional zero-paddings in the contour of the image. If we take zero-padding $P=1$, we would have
\begin{equation}
\begin{aligned}
(H-F+2*P)/S+1&=(10-3+2*1)/3+1=4~\text{(An integer)}\\
(W-F+2*P)/S+1&=(10-3+2*1)/3+1=4~\text{(An integer)}\\
\end{aligned}
\end{equation}
\end{itemize}

In general, setting zero padding to be $P=(F−1)/2$ when the stride is $S=1$ ensures that the input volume and output volume will have the same size spatially. It is very common to use zero-padding in this way.

\begin{figure}[!htb]
\centering
\includegraphics[height = 3in]{pics/cnn.png}
\caption{Convolution Demo\cite{cs231n-website}}
\label{fig:conv_demo}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[height = 2in]{pics/cnn2.png}
\caption{Number of filter and depth of output\cite{cs231n-website}}
\label{fig:conv_demo2}
\end{figure}
\subsection{Pooling Layer}
In practice, it is common to periodically insert a Pooling Layer in-between successive Conv layers in a ConvNet architecture. It is mainly used to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation load, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation if max pooling, AVG if average pooling, etc. The most common form is a pooling layer with filters of size $2\times2$ applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75\% of the activations. In this case, every pooling operation would be taking a max or average over 4 numbers (little $2\times2$ region in some depth slice). The depth dimension remains unchanged. In a nutshell, \textit{pooling layer \textbf{downsamples} the volume spatially, independently in \textbf{each} depth slice of the input volume.}

\begin{figure}[!htb]
\begin{subfigure}[!htb]{0.45\linewidth}
\includegraphics[scale=0.33]{pics/pool}
\caption{general pooling}
\label{fig:genpool}
\end{subfigure}
\hfill
\begin{subfigure}[!htb]{0.54\linewidth}
\includegraphics[scale=0.33]{pics/maxpool}
\caption{max pooling}
\label{fig:maxpool}
\end{subfigure}
\caption{Pooling Layer\cite{cs231n-website}}
\end{figure}

The Fig.\ref{fig:genpool} shows how pooling works generally. A $2\times2$ pooling filter with stride 2 is applied on the $224\times224\times64$ input volume, yielding the $112\times112\times64$ output volume. Note that the width and height of the input volume shrinks to half while the depth is preserved. The most common downsampling operation is MAX. The Fig.\ref{fig:maxpool} here shows max pooling with a stride of 2. That is, each max is taken over 4 numbers in a $2\times2$ square. For instance, the top-left red square squashes to a single value, \texttt{\textbf{6}}, which is \texttt{\textbf{max\{1, 1, 5, 6\}}}.

\subsection{Fully-Connected Layer}
The last few layers of a ConvNet architecture are typically Fully-Connected (FC) Layers. As seen in regular neural networks, FC Layers serve as a linear classifier for classification or regression. Their activations can be computed with a matrix multiplication followed by a bias offset, \textit{i.e.} $f = Wx + b$. See the Neural Network Basics section of the note for more information.

\subsection{Mainstream Model Architectures}
Empowered with impressive ability of feature extraction and optimization, deep convolutional neural networks has become the cornerstone of data-driven visual recognition techniques nowadays. Since AlexNet's overwhelming success in ILSVRC 2012, a number of milestone model architectures have been proposed. The most common are:
\begin{itemize}
\item \textbf{AlexNet}\cite{alexnet}: Developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet significantly outperformed the second runner-up (top 5 error of 16\% compared to runner-up with 26\% error) ILSVRC challenge in 2012. It is the first work that popularized Convolutional Neural Networks in Computer Vision.
\item \textbf{VGGNet}\cite{vggnet}: Proposed by Karen Simonyan and Andrew Zisserman. The VGGNet was the runner-up in ILSVRC 2014. Its main contribution was in showing that the depth of the network is a critical component for good performance. Researchers and engineers tend to design deeper rather than shallower models ever since.
\item \textbf{GoogLeNet}\cite{googlenet}: As its name shows, it is authored by Szegedy \textit{et al.} from Google. GoogLeNet was the ILSVRC 2014 winner with main contribution in developing Inception Module that dramatically reduced the number of parameters in the network.
\item \textbf{ResNet}\cite{resnet}:  Developed by Kaiming He \textit{et al.} ResNet was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization, which is now known as Residual Module. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of January 2018).
\end{itemize}

There also exist more model architectures proposed for specific tasks, such as YOLO\cite{yolo} and Faster R-CNN\cite{faster_rcnn} in the field of object detection and localization.

\section{Additional Resources}
\begin{enumerate}
    \item CS231n (CNNs for Visual Recognition) class notes : http://cs231n.github.io/convolutional-networks/
    \item Live Demo - Inner Workings of a CNN : http://scs.ryerson.ca/~aharley/vis/conv/
 2D version : http://scs.ryerson.ca/~aharley/vis/conv/flat.html
\item CS230 : https://cs230-stanford.github.io/
\item Tensorflow documentation : https://www.tensorflow.org/api_docs/python/tf
\end{enumerate}


% References
\bibliographystyle{unsrt}
\bibliography{references}

\subsubsection*{Contributors}
Chi Zhang, Honghao Wei, Matthew Tan, Taylor Howell, Brian Jackson, Saifan Rafiq

\end{document}
